"""
DocumentStore - main API for document storage and retrieval.

Uses shared AI utilities:
- ai.embeddings for vector creation
- ai.reranker for result reranking
- ai.vectordb for storage
- ai.tokens for context formatting
"""

import uuid
from typing import List, Dict, Any, Optional, Callable, Union
from dataclasses import dataclass, field


@dataclass
class Chunk:
    """A document chunk."""
    id: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    score: float = 0.0


@dataclass
class SearchResult:
    """Search results."""
    chunks: List[Chunk]
    query: str
    total: int


@dataclass
class AnswerResult:
    """Answer with sources."""
    answer: str
    sources: List[Chunk]
    confidence: float = 0.0


class DocumentStore:
    """
    Document store with search and QA.
    
    Usage:
        store = DocumentStore()
        
        # Add documents
        doc_id = await store.add(pdf_bytes, "contract.pdf", entity_id="prop_123")
        
        # Search
        results = await store.search("security deposit")
        
        # Answer (extractive QA by default)
        answer = await store.answer("what is the rent?")
        
        # Answer with custom LLM
        answer = await store.answer("summarize the lease", llm_fn=agent.chat)
    """
    
    def __init__(
        self,
        embed_fn: Callable = None,
        rerank_fn: Callable = None,
        preload_qa: bool = False,
    ):
        """
        Initialize document store.
        
        Args:
            embed_fn: Custom embedding function (auto-detected if None)
            rerank_fn: Custom rerank function (auto-detected if None)
            preload_qa: Preload tinyroberta QA model on init
        """
        # Use relative imports within backend.ai package
        from ..embeddings import Embedder
        from ..reranker import get_reranker
        from ..vectordb import MemoryStore
        
        # Create embedder instance and use its embed method
        if embed_fn is None:
            embedder = Embedder()
            self.embed_fn = embedder.embed
        else:
            self.embed_fn = embed_fn
        
        # Get reranker (may be None if loading fails)
        self.rerank_fn = rerank_fn if rerank_fn is not None else get_reranker()
        if self.rerank_fn is None:
            print("[DocumentStore] Reranking disabled (model load failed or not available)")
        
        self.vector_store = MemoryStore()  # Use concrete implementation
        
        # Document metadata
        self._documents: Dict[str, Dict[str, Any]] = {}
        
        # QA pipeline (lazy loaded)
        self._qa_pipeline = None
        self._qa_loading = False
        self._qa_error = None
        
        if preload_qa:
            self.preload_qa_model()
    
    def preload_qa_model(self):
        """Pre-load QA model in background."""
        import threading
        
        if self._qa_pipeline is not None or self._qa_loading:
            return
        
        def _load():
            try:
                self._qa_loading = True
                print("[DocumentStore] Pre-loading tinyroberta QA model...")
                from transformers import pipeline
                self._qa_pipeline = pipeline(
                    "question-answering", 
                    model="deepset/tinyroberta-squad2",
                    device=-1,
                )
                print("[DocumentStore] QA model ready")
            except Exception as e:
                self._qa_error = str(e)
                print(f"[DocumentStore] QA model failed: {e}")
            finally:
                self._qa_loading = False
        
        thread = threading.Thread(target=_load, daemon=True)
        thread.start()
    
    def get_qa_status(self) -> dict:
        """Get QA model status."""
        return {
            "ready": self._qa_pipeline is not None,
            "loading": self._qa_loading,
            "error": self._qa_error,
        }
    
    async def add(
        self,
        file_bytes: bytes,
        filename: str,
        entity_id: str = None,
        tags: List[str] = None,
        metadata: Dict[str, Any] = None,
    ) -> str:
        """
        Add a document to the store.
        
        Args:
            file_bytes: File content
            filename: Original filename
            entity_id: Optional entity ID (e.g., property_id, agent_id)
            tags: Optional tags for filtering
            metadata: Additional metadata
            
        Returns:
            Document ID (generated by store)
        """
        from .ingestion import extract_file, chunk_text
        from ..vectordb import Document
        
        metadata = metadata or {}
        if entity_id:
            metadata["entity_id"] = entity_id
        if tags:
            metadata["tags"] = tags
        
        # Extract text
        text = extract_file(file_bytes, filename)
        
        if not text.strip():
            raise ValueError(f"No text extracted from {filename}")
        
        # Chunk
        chunks = chunk_text(text)
        
        # Generate document ID
        doc_id = str(uuid.uuid4())[:12]
        metadata["document_id"] = doc_id  # Always set for downstream lookup
        
        # Store document metadata
        self._documents[doc_id] = {
            "filename": filename,
            "chunk_count": len(chunks),
            "metadata": metadata,
        }
        
        # Embed and store
        if chunks:
            embeddings = self.embed_fn(chunks)
            
            docs = []
            for i, (content, embedding) in enumerate(zip(chunks, embeddings)):
                chunk_id = f"{doc_id}_{i:04d}"
                docs.append(Document(
                    id=chunk_id,
                    content=content,
                    embedding=embedding,
                    metadata={
                        **metadata,  # Includes document_id, entity_id, tags
                        "doc_id": doc_id,  # Legacy alias
                        "filename": filename,
                        "chunk_index": i,
                    },
                ))
            
            await self.vector_store.save(docs)
        
        return doc_id
    
    async def add_text(
        self,
        text: str,
        doc_id: str = None,
        metadata: Dict[str, Any] = None,
    ) -> str:
        """Add raw text directly."""
        from .ingestion import chunk_text
        from ..vectordb import Document
        
        doc_id = doc_id or str(uuid.uuid4())[:12]
        metadata = metadata or {}
        
        chunks = chunk_text(text)
        
        self._documents[doc_id] = {
            "filename": f"{doc_id}.txt",
            "chunk_count": len(chunks),
            "metadata": metadata,
        }
        
        if chunks:
            embeddings = self.embed_fn(chunks)
            
            docs = []
            for i, (content, embedding) in enumerate(zip(chunks, embeddings)):
                chunk_id = f"{doc_id}_{i:04d}"
                docs.append(Document(
                    id=chunk_id,
                    content=content,
                    embedding=embedding,
                    metadata={**metadata, "doc_id": doc_id, "chunk_index": i},
                ))
            
            await self.vector_store.save(docs)
        
        return doc_id
    
    async def search(
        self,
        query: str,
        top_k: int = 5,
        filters: Dict[str, Any] = None,
        tags: List[str] = None,
        rerank: bool = True,
    ) -> SearchResult:
        """
        Search for relevant chunks.
        
        Args:
            query: Search query
            top_k: Number of results
            filters: Metadata filters (exact match)
            tags: Filter by tags (match ANY tag)
            rerank: Whether to use cross-encoder reranking
            
        Returns:
            SearchResult with chunks
        """
        from ..reranker import is_language_supported
        
        # Embed query
        query_embedding = self.embed_fn(query)
        
        # Build combined filters (including tags filter if specified)
        combined_filters = dict(filters) if filters else {}
        # Note: tags filtering would need special handling in vector store
        # For now, we filter tags post-retrieval if needed
        
        print(f"[DEBUG DocumentStore.search] query='{query}', filters={combined_filters}")
        
        # Search
        fetch_k = top_k * 4 if rerank else top_k
        results = await self.vector_store.search(
            query_embedding=query_embedding,
            top_k=fetch_k,
            filters=combined_filters,
        )
        
        print(f"[DEBUG DocumentStore.search] Got {len(results.documents)} documents from vector store")
        
        # Filter by tags if specified (post-retrieval)
        if tags and results.documents:
            results.documents = [
                d for d in results.documents
                if any(t in d.metadata.get("tags", []) for t in tags)
            ]
        
        if not results.documents:
            return SearchResult(chunks=[], query=query, total=0)
        
        # Rerank if enabled and language supported
        if rerank and self.rerank_fn and len(results.documents) > top_k:
            if is_language_supported(query):
                texts = [d.content for d in results.documents]
                ranked = self.rerank_fn(query, texts, top_k=top_k)
                
                chunks = []
                for idx, score in ranked:
                    doc = results.documents[idx]
                    chunks.append(Chunk(
                        id=doc.id,
                        content=doc.content,
                        metadata=doc.metadata,
                        score=float(score),
                    ))
            else:
                # Language not supported, use cosine scores
                chunks = [
                    Chunk(id=d.id, content=d.content, metadata=d.metadata, score=d.score)
                    for d in results.documents[:top_k]
                ]
        else:
            chunks = [
                Chunk(id=d.id, content=d.content, metadata=d.metadata, score=d.score)
                for d in results.documents[:top_k]
            ]
        
        return SearchResult(chunks=chunks, query=query, total=results.total)
    
    async def answer(
        self,
        question: str,
        top_k: int = 5,
        filters: Dict[str, Any] = None,
        tags: List[str] = None,
        llm_fn: Callable = None,
    ) -> AnswerResult:
        """
        Answer a question using retrieved context.
        
        Args:
            question: Question to answer
            top_k: Number of chunks to retrieve
            filters: Metadata filters (exact match)
            tags: Filter by tags (match ANY tag)
            llm_fn: Optional LLM function. If None, uses extractive QA.
                    Expected: async fn(messages) -> response with .content
                    
        Returns:
            AnswerResult with answer and sources
        """
        # Search
        results = await self.search(question, top_k=top_k, filters=filters, tags=tags)
        
        if not results.chunks:
            return AnswerResult(
                answer="I couldn't find relevant information to answer this question.",
                sources=[],
                confidence=0.0,
            )
        
        # Build context
        context = self._format_context(results.chunks)
        
        if llm_fn:
            # Use provided LLM
            messages = [
                {"role": "system", "content": self._get_rag_prompt(context)},
                {"role": "user", "content": question},
            ]
            response = await llm_fn(messages)
            answer = response.content if hasattr(response, 'content') else str(response)
            confidence = 0.8
        else:
            # Use extractive QA
            answer, confidence = self._extract_answer(question, context)
        
        return AnswerResult(
            answer=answer,
            sources=results.chunks,
            confidence=confidence,
        )
    
    async def delete(self, doc_id: str) -> bool:
        """Delete a document and its chunks."""
        if doc_id not in self._documents:
            return False
        
        await self.vector_store.delete_by_filter({"doc_id": doc_id})
        del self._documents[doc_id]
        return True
    
    def list_documents(self, tags: List[str] = None) -> List[Dict[str, Any]]:
        """
        List all documents, optionally filtered by tags.
        
        Args:
            tags: Filter by tags (match ANY tag)
        """
        docs = []
        for doc_id, doc in self._documents.items():
            if tags:
                doc_tags = doc.get("metadata", {}).get("tags", [])
                if not any(t in doc_tags for t in tags):
                    continue
            docs.append({"id": doc_id, **doc})
        return docs
    
    async def add_tags(self, doc_id: str, tags: List[str]) -> bool:
        """Add tags to a document."""
        if doc_id not in self._documents:
            return False
        
        # Update document metadata
        if "metadata" not in self._documents[doc_id]:
            self._documents[doc_id]["metadata"] = {}
        
        existing = self._documents[doc_id]["metadata"].get("tags", [])
        new_tags = list(set(existing + tags))
        self._documents[doc_id]["metadata"]["tags"] = new_tags
        
        # Update all chunks in vector store
        await self.vector_store.update_by_filter(
            filters={"doc_id": doc_id},
            metadata={"tags": new_tags},
        )
        
        return True
    
    async def remove_tags(self, doc_id: str, tags: List[str]) -> bool:
        """Remove tags from a document."""
        if doc_id not in self._documents:
            return False
        
        existing = self._documents[doc_id].get("metadata", {}).get("tags", [])
        new_tags = [t for t in existing if t not in tags]
        self._documents[doc_id]["metadata"]["tags"] = new_tags
        
        # Update all chunks in vector store
        await self.vector_store.update_by_filter(
            filters={"doc_id": doc_id},
            metadata={"tags": new_tags},
        )
        
        return True
    
    def get_all_tags(self) -> List[str]:
        """Get all unique tags across all documents."""
        tags = set()
        for doc in self._documents.values():
            doc_tags = doc.get("metadata", {}).get("tags", [])
            tags.update(doc_tags)
        return sorted(tags)
    
    async def count(self, filters: Dict[str, Any] = None) -> int:
        """Count chunks."""
        return await self.vector_store.count(filters)
    
    async def count_unique_documents(self, filters: Dict[str, Any] = None) -> int:
        """
        Count unique documents (not chunks).
        
        This counts distinct document_id values in the vector store metadata.
        """
        # Get all chunks matching filters (or all if no filters)
        # We use a large top_k to get all chunks, but only need metadata
        try:
            # Search with empty query to get all matching documents
            all_chunks = await self.vector_store.query(
                vector=[0.0] * 384,  # Dummy vector - we just need metadata
                top_k=10000,  # Large enough to get all
                filters=filters,
            )
            
            # Count unique document_ids
            unique_docs = set()
            for chunk in all_chunks:
                doc_id = chunk.get("metadata", {}).get("document_id") or chunk.get("metadata", {}).get("doc_id")
                if doc_id:
                    unique_docs.add(doc_id)
            
            return len(unique_docs)
        except Exception as e:
            print(f"[WARN DocumentStore.count_unique_documents] Failed: {e}")
            # Fallback to in-memory count
            if filters and "entity_id" in filters:
                entity_id = filters["entity_id"]
                return sum(
                    1 for doc in self._documents.values()
                    if doc.get("metadata", {}).get("entity_id") == entity_id
                )
            return len(self._documents)
    
    async def clear(self):
        """Clear all documents."""
        await self.vector_store.clear()
        self._documents.clear()
    
    # =========================================================================
    # Private methods
    # =========================================================================
    
    def _format_context(self, chunks: List[Chunk], max_tokens: int = 3000) -> str:
        """Format chunks into context string."""
        from ..tokens import estimate_tokens
        
        parts = []
        used_tokens = 0
        
        for i, chunk in enumerate(chunks, 1):
            source = chunk.metadata.get("filename", f"Source {i}")
            page = chunk.metadata.get("page_num", "")
            
            if page:
                header = f"[{source}, p.{page}]"
            else:
                header = f"[{source}]"
            
            text = f"{header}\n{chunk.content}"
            tokens = estimate_tokens(text)
            
            if used_tokens + tokens > max_tokens:
                break
            
            parts.append(text)
            used_tokens += tokens
        
        return "\n\n---\n\n".join(parts)
    
    def _get_rag_prompt(self, context: str) -> str:
        """Get RAG system prompt."""
        return f"""Answer based on this context. If the answer is not in the context, say "I don't know".

Context:
{context}"""
    
    def _extract_answer(self, question: str, context: str) -> tuple:
        """
        Extract answer using tinyroberta.
        
        Note: TinyRoberta has 512 token limit, so context is truncated.
        """
        try:
            from transformers import pipeline
            
            # Cache the pipeline
            if not hasattr(self, '_qa_pipeline') or self._qa_pipeline is None:
                print("[DocumentStore] Loading tinyroberta QA model...")
                self._qa_pipeline = pipeline(
                    "question-answering", 
                    model="deepset/tinyroberta-squad2",
                    device=-1,  # CPU
                )
                print("[DocumentStore] QA model loaded")
            
            # TinyRoberta has ~512 token context limit
            # Truncate context to ~400 tokens (~1600 chars) to leave room for question
            max_context_chars = 1600
            if len(context) > max_context_chars:
                context = context[:max_context_chars] + "..."
            
            result = self._qa_pipeline(question=question, context=context)
            
            answer = result.get("answer", "").strip()
            score = result.get("score", 0.0)
            
            print(f"[DocumentStore] QA result: answer='{answer[:50]}...' score={score:.3f}")
            
            if not answer or score < 0.05:
                return "I couldn't find a clear answer in the provided documents.", score
            
            return answer, score
            
        except ImportError:
            raise ImportError("Install transformers: pip install transformers")
        except Exception as e:
            print(f"[DocumentStore] QA error: {e}")
            return f"Error extracting answer: {str(e)}", 0.0
