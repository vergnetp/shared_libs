# AI Agents Module - Complete Design Plan

A provider-agnostic Python module for creating and managing AI agents with persistent state, built on top of existing RAG infrastructure.

---

## 1. Executive Summary

### What We're Building

An **"OpenAI Assistants API, but provider-agnostic and self-hosted"** that:

- Works with **any LLM provider** (OpenAI, Anthropic, Ollama, etc.)
- **Our database is the source of truth** (not OpenAI's 60-day expiring threads)
- Integrates with **existing RAG system** (OpenSearch, embeddings, reranker)
- Uses **Redis for queuing and rate limiting** from day one
- Supports **multiple memory strategies** (sliding window, summarize, vector)
- Enables **seamless provider switching** (cloud today, local Ollama tomorrow)

### Tech Stack

| Component | Technology | Status |
|-----------|------------|--------|
| Vector Search | OpenSearch | âœ… Existing |
| Embeddings | MiniLM (model_hub) | âœ… Existing |
| Reranking | Cross-encoder + MMR | âœ… Existing |
| Document Ingestion | chunk_creator + doc_ingestor | âœ… Existing |
| Database | PostgreSQL | âœ… Existing |
| Queue & Rate Limiting | Redis | ðŸ”¨ New |
| Agent Framework | This module | ðŸ”¨ New |

---

## 2. Core Philosophy

### The Problem with OpenAI Assistants

| Aspect | OpenAI Assistants | Problem |
|--------|-------------------|---------|
| Thread storage | Their servers | 60-day expiry, no control |
| Thread listing | Not available | You track IDs yourself |
| Cross-thread memory | Not available | Build yourself |
| Provider lock-in | 100% | Can't switch to Anthropic/Ollama |
| Memory strategy | `auto` or `last_messages` only | No summarize, no vector |
| Data ownership | Theirs | Compliance, privacy concerns |

### Our Solution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OUR SYSTEM (Full Control)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL: Agents, Threads, Messages (source of truth)    â”‚
â”‚  Redis: Rate limiting, job queue, caching                   â”‚
â”‚  OpenSearch: Document RAG (existing)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Re-inject context as needed
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLM PROVIDER (Compute Only)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OpenAI / Anthropic / Ollama / Groq / etc.                  â”‚
â”‚  â€¢ Run inference                                            â”‚
â”‚  â€¢ Return response                                          â”‚
â”‚  â€¢ OpenAI thread = disposable cache (optional)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key insight:** OpenAI Assistants API becomes an **optional optimization**, not a dependency. We can use it when convenient (native tool calling, token caching) but don't rely on it for persistence.

---

## 3. Architecture Overview

### Module Structure

```
ai_agents/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ definition.py              # AgentDefinition dataclass
â”œâ”€â”€ agent.py                   # Main Agent class
â”œâ”€â”€ runner.py                  # Execution loop
â”œâ”€â”€ store.py                   # AgentStore (PostgreSQL operations)
â”‚
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # Abstract provider interface
â”‚   â”œâ”€â”€ openai_assistants.py   # Uses Assistants API (optional)
â”‚   â”œâ”€â”€ openai_completions.py  # Stateless completions
â”‚   â”œâ”€â”€ anthropic.py           # Stateless
â”‚   â”œâ”€â”€ ollama.py              # Stateless + tool injection
â”‚   â””â”€â”€ registry.py            # Model limits registry
â”‚
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # Abstract memory interface
â”‚   â”œâ”€â”€ strategies.py          # last_messages, summarize, vector, etc.
â”‚   â””â”€â”€ context.py             # Context window management
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ decorator.py           # @tool decorator
â”‚   â”œâ”€â”€ registry.py            # Tool registry
â”‚   â”œâ”€â”€ parser.py              # Parse tool calls from text
â”‚   â””â”€â”€ builtin.py             # Built-in RAG tools
â”‚
â”œâ”€â”€ queue/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ redis_queue.py         # Redis job queue
â”‚   â”œâ”€â”€ rate_limiter.py        # Redis-based rate limiting
â”‚   â””â”€â”€ token_counter.py       # Token counting utilities
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ exceptions.py
```

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              CLIENT                                      â”‚
â”‚                     agent.chat("What's the refund policy?")              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           AGENT RUNNER                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load agent definition + thread history from PostgreSQL              â”‚
â”‚  2. Apply memory strategy (fit context window)                          â”‚
â”‚  3. Check rate limits (Redis)                                           â”‚
â”‚  4. Call LLM provider                                                   â”‚
â”‚  5. Execute tools if requested (loop)                                   â”‚
â”‚  6. Save messages to PostgreSQL                                         â”‚
â”‚  7. Return response                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚  â”‚     Redis       â”‚  â”‚   LLM Provider  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ agents        â”‚  â”‚ â€¢ rate limits   â”‚  â”‚ â€¢ OpenAI        â”‚
â”‚ â€¢ threads       â”‚  â”‚ â€¢ job queue     â”‚  â”‚ â€¢ Anthropic     â”‚
â”‚ â€¢ messages      â”‚  â”‚ â€¢ token counts  â”‚  â”‚ â€¢ Ollama        â”‚
â”‚ â€¢ user_memory   â”‚  â”‚ â€¢ caching       â”‚  â”‚ â€¢ Groq          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXISTING RAG (OpenSearch)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  vectordb.py â†’ searcher.py â†’ reranker.py â†’ doc_ingestor.py              â”‚
â”‚  (Wrapped as agent tools)                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Database Schema

### PostgreSQL Tables

```sql
-- ============================================
-- AGENTS (like OpenAI's assistants.create)
-- ============================================
CREATE TABLE agents (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name            TEXT NOT NULL,
    
    -- Definition (compiles to system prompt)
    role            TEXT NOT NULL,
    goal            TEXT,
    constraints     JSONB,              -- ["constraint 1", "constraint 2"]
    personality     JSONB,              -- {"tone": "friendly", "style": "concise"}
    examples        JSONB,              -- [{"user": "...", "assistant": "..."}]
    
    -- Tools
    tools           JSONB,              -- ["search_documents", "ask_documents"]
    
    -- Provider config
    provider        TEXT NOT NULL,      -- "openai", "anthropic", "ollama"
    model           TEXT NOT NULL,      -- "gpt-4o", "claude-3.5-sonnet"
    
    -- Memory config
    memory_strategy TEXT DEFAULT 'last_messages',
    memory_config   JSONB,              -- {"max_messages": 50, "preserve_first": 3}
    
    -- Metadata
    created_at      TIMESTAMPTZ DEFAULT NOW(),
    updated_at      TIMESTAMPTZ DEFAULT NOW(),
    created_by      TEXT,               -- user_id who created
    is_active       BOOLEAN DEFAULT TRUE
);

-- ============================================
-- THREADS (like OpenAI's threads.create)
-- ============================================
CREATE TABLE threads (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_id        UUID REFERENCES agents(id) ON DELETE CASCADE,
    user_id         TEXT,               -- For multi-tenant
    
    -- Size tracking
    total_bytes     BIGINT DEFAULT 0,
    message_count   INTEGER DEFAULT 0,
    
    -- Timestamps
    created_at      TIMESTAMPTZ DEFAULT NOW(),
    last_used_at    TIMESTAMPTZ DEFAULT NOW(),
    
    -- OpenAI cache (optional, can be null/stale)
    openai_thread_id TEXT,
    openai_thread_created_at TIMESTAMPTZ,
    
    -- Status
    archived        BOOLEAN DEFAULT FALSE,
    metadata        JSONB
);

-- ============================================
-- MESSAGES (like OpenAI's messages.create)
-- ============================================
CREATE TABLE messages (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    thread_id       UUID REFERENCES threads(id) ON DELETE CASCADE,
    
    -- Content
    role            TEXT NOT NULL,      -- "system", "user", "assistant", "tool"
    content         TEXT,
    
    -- Tool calls (for assistant messages)
    tool_calls      JSONB,              -- [{"id": "...", "name": "...", "arguments": {...}}]
    
    -- Tool response (for tool messages)
    tool_call_id    TEXT,
    tool_name       TEXT,
    
    -- Metadata
    created_at      TIMESTAMPTZ DEFAULT NOW(),
    token_count     INTEGER,            -- Cached token count
    metadata        JSONB
);

-- ============================================
-- USER MEMORY (cross-thread, OpenAI doesn't have this)
-- ============================================
CREATE TABLE user_memory (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id         TEXT NOT NULL,
    agent_id        UUID REFERENCES agents(id) ON DELETE CASCADE,
    
    key             TEXT NOT NULL,
    value           TEXT,
    
    created_at      TIMESTAMPTZ DEFAULT NOW(),
    updated_at      TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE(user_id, agent_id, key)
);

-- ============================================
-- INDEXES
-- ============================================
CREATE INDEX idx_threads_agent ON threads(agent_id);
CREATE INDEX idx_threads_user ON threads(user_id);
CREATE INDEX idx_threads_last_used ON threads(last_used_at);
CREATE INDEX idx_messages_thread ON messages(thread_id);
CREATE INDEX idx_messages_created ON messages(created_at);
CREATE INDEX idx_user_memory_user ON user_memory(user_id, agent_id);
```

### Size Limits

| Level | Limit | Rationale |
|-------|-------|-----------|
| Thread size | 50 MB | Prevent runaway threads |
| User total | 500 MB | Fair usage per user |
| Tenant total | Configurable | Billing tier (Hostomatic) |

---

## 5. Agent Definition

### Data Structure

```python
from dataclasses import dataclass, field
from typing import Optional

@dataclass
class AgentDefinition:
    """
    Defines an agent's identity, goals, constraints, and behavior.
    Compiles to a system prompt for LLM providers.
    """
    name: str
    role: str                                    # "You are a chess teacher"
    goal: Optional[str] = None                   # "Help users improve..."
    constraints: list[str] = field(default_factory=list)
    personality: Optional[dict] = None           # {"tone": "friendly"}
    examples: list[dict] = field(default_factory=list)
    tools: list[str] = field(default_factory=list)
    
    def compile(self) -> str:
        """Generate system prompt from definition."""
        sections = []
        
        # Role (required)
        sections.append(f"# Role\n{self.role}")
        
        # Goal
        if self.goal:
            sections.append(f"# Goal\n{self.goal}")
        
        # Constraints
        if self.constraints:
            items = "\n".join(f"- {c}" for c in self.constraints)
            sections.append(f"# Constraints\n{items}")
        
        # Personality
        if self.personality:
            tone = self.personality.get("tone", "professional")
            style = self.personality.get("style", "")
            sections.append(f"# Communication Style\nTone: {tone}")
            if style:
                sections.append(f"Style: {style}")
        
        # Examples (few-shot)
        if self.examples:
            examples_text = self._format_examples()
            sections.append(f"# Example Interactions\n{examples_text}")
        
        return "\n\n".join(sections)
```

### Usage Example

```python
hostomatic_assistant = AgentDefinition(
    name="Property Assistant",
    role="You are a helpful property management assistant for vacation rental hosts.",
    goal="Help hosts manage their properties efficiently by answering questions, "
         "searching documents, and providing actionable advice.",
    constraints=[
        "Always cite which document information comes from",
        "If you're unsure, say so rather than guessing",
        "Respect guest privacy - don't share personal details",
        "For legal or tax questions, recommend consulting a professional",
    ],
    personality={"tone": "friendly and professional", "style": "concise but thorough"},
    tools=["search_documents", "ask_documents", "list_properties"],
)
```

---

## 6. Provider Abstraction

### Provider Selection Logic

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Is provider OpenAI AND memory strategy native-compatible?   â”‚
â”‚ (last_messages or auto)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â–¼             â–¼
    [YES]          [NO]
       â”‚             â”‚
       â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Use OpenAI  â”‚  â”‚ Use Stateless       â”‚
â”‚ Assistants  â”‚  â”‚ (re-inject every    â”‚
â”‚ API         â”‚  â”‚  call)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Base Provider Interface

```python
from abc import ABC, abstractmethod

class BaseProvider(ABC):
    """Abstract interface for LLM providers."""
    
    @abstractmethod
    async def run(
        self,
        system_prompt: str,
        messages: list[Message],
        tools: list[Tool] | None = None,
    ) -> ProviderResponse:
        ...
    
    @property
    @abstractmethod
    def supports_native_tools(self) -> bool:
        ...
```

### Tool Injection for Non-Native Providers (Ollama)

```python
TOOL_INJECTION_TEMPLATE = """
## Available Tools

You have access to the following tools:

{tool_definitions}

To use a tool, respond with EXACTLY this XML format:

<tool_call>
<n>tool_name_here</n>
<arguments>
{{"param1": "value1", "param2": "value2"}}
</arguments>
</tool_call>

Rules:
- Call only ONE tool at a time
- Wait for the result before calling another tool
- If you don't need a tool, respond normally without XML tags
"""
```

---

## 7. Model Limits Registry

```python
@dataclass
class ModelLimits:
    context_window: int      # Total tokens (input + output)
    max_output: int          # Max output tokens
    rpm: int                 # Requests per minute
    tpm: int                 # Tokens per minute
    supports_tools: bool
    supports_vision: bool
    cost_per_1m_input: float
    cost_per_1m_output: float


MODEL_REGISTRY = {
    "openai": {
        "gpt-4.1": ModelLimits(1_000_000, 32_000, 500, 30_000, True, True, 2.00, 8.00),
        "gpt-4o": ModelLimits(128_000, 16_000, 500, 30_000, True, True, 2.50, 10.00),
        "gpt-4o-mini": ModelLimits(128_000, 16_000, 500, 30_000, True, True, 0.15, 0.60),
    },
    "anthropic": {
        "claude-sonnet-4-20250514": ModelLimits(200_000, 8_000, 50, 40_000, True, True, 3.00, 15.00),
        "claude-3-5-sonnet-20241022": ModelLimits(200_000, 8_000, 50, 40_000, True, True, 3.00, 15.00),
        "claude-3-haiku-20240307": ModelLimits(200_000, 4_000, 50, 40_000, True, True, 0.25, 1.25),
    },
    "ollama": {
        "llama3.3:70b": ModelLimits(128_000, 2_000, 999_999, 999_999_999, False, False, 0, 0),
        "qwen2.5:72b": ModelLimits(128_000, 8_000, 999_999, 999_999_999, False, False, 0, 0),
    },
    "deepseek": {
        "deepseek-chat": ModelLimits(64_000, 8_000, 60, 100_000, True, False, 0.14, 0.28),
    },
    "google": {
        "gemini-2.0-flash": ModelLimits(1_000_000, 8_000, 60, 4_000_000, True, True, 0.075, 0.30),
        "gemini-2.5-pro": ModelLimits(1_000_000, 64_000, 60, 4_000_000, True, True, 1.25, 5.00),
    },
}
```

---

## 8. Memory Strategies

| Strategy | Description | Use Case | OpenAI Native |
|----------|-------------|----------|---------------|
| `last_messages` | Keep last N messages | General chat | âœ… Yes |
| `first_last` | Keep first M + last N | Preserve setup context | âŒ No |
| `summarize` | Compress old â†’ summary | Very long conversations | âŒ No |
| `vector` | Retrieve relevant chunks | Knowledge-heavy agents | âŒ No |

```python
class MemoryStrategy(ABC):
    @abstractmethod
    def prepare_messages(
        self,
        system_prompt: str,
        history: list[Message],
        user_input: str,
        limits: ModelLimits,
    ) -> list[Message]:
        ...
    
    @property
    @abstractmethod
    def is_openai_native_compatible(self) -> bool:
        ...
```

---

## 9. Redis Integration

### Why Redis from Day One

| Requirement | Without Redis | With Redis |
|-------------|---------------|------------|
| Rate limiting across workers | âŒ Can't coordinate | âœ… Shared state |
| Job queue for long tasks | âŒ Block request | âœ… Background processing |
| Token counting across requests | âŒ Per-process only | âœ… Accurate TPM tracking |
| Scale to multiple servers | âŒ Impossible | âœ… Ready |

### Redis Keys

```python
# Rate limiting (sorted sets with timestamps)
RATE_KEY_RPM = "ratelimit:{provider}:{model}:rpm"
RATE_KEY_TPM = "ratelimit:{provider}:{model}:tpm"

# Job queue
JOB_QUEUE = "jobs:agent_runs"
JOB_RESULTS = "jobs:results:{job_id}"

# Caching
CACHE_THREAD = "cache:thread:{thread_id}"
CACHE_AGENT = "cache:agent:{agent_id}"
```

### Rate Limiter

```python
class RedisRateLimiter:
    """Sliding window rate limiter using Redis sorted sets."""
    
    async def wait_if_needed(self, estimated_tokens: int) -> float:
        """Wait if rate limits would be exceeded. Returns seconds waited."""
        ...
    
    async def record_request(self, tokens_used: int):
        """Record a completed request for rate tracking."""
        ...
```

### Job Queue

```python
class RedisJobQueue:
    """Simple job queue for background agent runs using Redis streams."""
    
    async def enqueue(self, thread_id: str, user_input: str) -> str:
        """Add job to queue, return job_id."""
        ...
    
    async def dequeue(self, consumer_name: str) -> dict | None:
        """Get next job (blocking)."""
        ...
    
    async def get_result(self, job_id: str, timeout: int = 30) -> dict | None:
        """Poll for job result."""
        ...
```

---

## 10. Thread Management

### OpenAI Thread as Disposable Cache

OpenAI threads expire after 60 days. We handle this transparently:

```python
class ThreadManager:
    async def get_or_create_openai_thread(self, thread_id: str) -> str | None:
        """Get valid OpenAI thread ID, recreating if expired."""
        
        thread = await self.db.get_thread(thread_id)
        
        # Recent enough to trust?
        if thread.openai_thread_id and thread.last_used_at:
            age = datetime.utcnow() - thread.last_used_at
            if age < timedelta(days=50):
                return thread.openai_thread_id
        
        # Verify or recreate
        if await self._is_valid_openai_thread(thread.openai_thread_id):
            return thread.openai_thread_id
        
        return await self._recreate_openai_thread(thread_id)
    
    async def _recreate_openai_thread(self, thread_id: str) -> str:
        """Rebuild OpenAI thread from our database."""
        messages = await self._get_messages(thread_id)
        
        openai_thread = await self.openai.beta.threads.create(
            messages=[{"role": m.role, "content": m.content} for m in messages]
        )
        
        await self.db.update_thread(thread_id, openai_thread_id=openai_thread.id)
        return openai_thread.id
```

---

## 11. Tool System

### Tool Decorator

```python
@tool(description="Search documents for relevant information")
async def search_documents(
    query: str,
    entity_id: str = None,
    top_k: int = 5,
) -> list[dict]:
    """Search the document knowledge base."""
    result = search_only(question=query, entity_id=entity_id, final_chunks=top_k)
    return result.get("hits", [])
```

### Built-in RAG Tools (Wrapping Existing System)

```python
# Wrap your existing RAG as agent tools

@tool(description="Search documents for information")
async def search_documents(query: str, entity_id: str = None, top_k: int = 5):
    """Uses existing searcher.search_only()"""
    ...

@tool(description="Ask a question and get an answer from documents")
async def ask_documents(question: str, entity_id: str = None):
    """Uses existing searcher.search()"""
    ...

@tool(description="Upload a document to the knowledge base")
async def upload_document(file_path: str, entity_id: str = None):
    """Uses existing doc_ingestor.ingest_file()"""
    ...

@tool(description="Delete a document from the knowledge base")
async def delete_document(file_hash: str):
    """Uses existing vectordb.delete_by_file_hash()"""
    ...

@tool(description="List all documents in the knowledge base")
async def list_documents(entity_id: str = None):
    """Uses existing vectordb.get()"""
    ...
```

---

## 12. Execution Flow

```
User Input â†’ Load Thread/Agent â†’ Apply Memory Strategy â†’ 
Check Rate Limits (Redis) â†’ Call LLM â†’ Tool Calls? â†’
[YES: Execute Tools â†’ Loop] / [NO: Continue] â†’
Record Usage (Redis) â†’ Save Messages (PostgreSQL) â†’ Response
```

### Agent Runner

```python
class AgentRunner:
    async def run(self, thread_id: str, user_input: str) -> AgentResponse:
        # 1. Load data from PostgreSQL
        thread = await self.store.get_thread(thread_id)
        agent = await self.store.get_agent(thread.agent_id)
        history = await self.store.get_messages(thread_id)
        
        # 2. Compile system prompt
        system_prompt = agent.definition.compile()
        
        # 3. Apply memory strategy
        messages = strategy.prepare_messages(system_prompt, history, user_input, limits)
        
        # 4. Tool execution loop
        for iteration in range(max_tool_iterations):
            # Check rate limits (Redis)
            await rate_limiter.wait_if_needed(estimated_tokens)
            
            # Call provider
            response = await provider.run(system_prompt, messages, tools)
            
            # Record usage (Redis)
            await rate_limiter.record_request(response.usage["total_tokens"])
            
            # No tool calls? Done
            if not response.tool_calls:
                break
            
            # Execute tools and loop
            results = await self._execute_tools(response.tool_calls)
            messages.extend(...)
        
        # 5. Save to PostgreSQL
        await self.store.add_message(thread_id, "user", user_input)
        await self.store.add_message(thread_id, "assistant", response.content)
        
        return response
```

---

## 13. Public API

### Simple Usage

```python
from ai_agents import Agent

agent = Agent(
    name="Property Assistant",
    role="You help property managers with their vacation rentals.",
    provider="openai",
    model="gpt-4o",
    tools=["search_documents", "ask_documents"],
)

response = await agent.chat("What documents do I have uploaded?")
```

### Full Control

```python
from ai_agents import AgentStore, AgentDefinition

store = AgentStore(
    database_url="postgresql://...",
    redis_url="redis://localhost:6379",
)

agent_id = await store.create_agent(
    definition=AgentDefinition(...),
    provider="anthropic",
    model="claude-sonnet-4-20250514",
    memory_strategy="summarize",
)

thread_id = await store.create_thread(agent_id=agent_id, user_id="user_123")

response = await store.run(thread_id, "What's the checkout time?")

# Switch provider later
await store.update_agent(agent_id, provider="ollama", model="llama3.3:70b")
```

---

## 14. Features Beyond OpenAI

| Feature | OpenAI Assistants | Our Module |
|---------|-------------------|------------|
| Thread listing | âŒ Track IDs yourself | âœ… Query database |
| Cross-thread memory | âŒ Not available | âœ… `user_memory` table |
| Fork/branch threads | âŒ Manual copy | âœ… Built-in |
| Custom truncation | âŒ `auto` or `last_messages` | âœ… Any strategy |
| Thread archiving | âŒ Manual before 60 days | âœ… Forever (your DB) |
| Provider switching | âŒ Locked to OpenAI | âœ… Change anytime |
| Cost tracking | âŒ Check billing | âœ… Per-thread, per-user |
| Rate limit coordination | âŒ Per-process | âœ… Redis shared |
| Background jobs | âŒ Not available | âœ… Redis queue |

---

## 15. Implementation Phases

| Phase | Scope | Week |
|-------|-------|------|
| **1. Foundation** | AgentDefinition, PostgreSQL schema, AgentStore CRUD | 1 |
| **2. Redis** | Connection, RateLimiter, token tracking | 1-2 |
| **3. Providers** | Base interface, OpenAI, Anthropic, registry | 2 |
| **4. Memory** | Strategies (last_messages, first_last, summarize) | 3 |
| **5. Tools** | Decorator, registry, RAG wrappers | 3-4 |
| **6. Runner** | Execution loop, tool calls, persistence | 4 |
| **7. OpenAI Assistants** | Optional native provider, thread recreation | 5 |
| **8. Ollama** | Tool injection, response parsing | 5 |
| **9. Job Queue** | RedisJobQueue, workers, status tracking | 6 |
| **10. Polish** | Error handling, logging, tests, docs | 6+ |

---

## 16. Dependencies

```toml
[project]
dependencies = [
    # Core
    "pydantic>=2.0",
    "sqlalchemy>=2.0",
    "asyncpg",
    "redis>=5.0",
    "tiktoken",
    
    # Providers
    "openai>=1.0",
    "anthropic>=0.20",
    "httpx",
    
    # Existing (already installed)
    # opensearch-py, torch, transformers, sentence-transformers
]
```

---

## 17. Summary

This module provides:

1. **Provider-agnostic agents** - Same code works with OpenAI, Anthropic, Ollama
2. **Full data ownership** - PostgreSQL as source of truth, no vendor lock-in
3. **Redis-powered scaling** - Rate limiting, job queue, caching from day one
4. **Existing RAG integration** - Wrap current OpenSearch/embeddings as tools
5. **Flexible memory** - Multiple strategies beyond OpenAI's limitations
6. **Future-proof** - Ready for local LLMs when hardware catches up

**Key insight:** OpenAI Assistants API is a nice optimization, not a dependency. We build the persistence layer ourselves and use providers purely for compute.
